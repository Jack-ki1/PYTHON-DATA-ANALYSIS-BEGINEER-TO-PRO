{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad21c34-fb52-4960-a03a-e743bf639890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5154687f-ffd1-47e2-b2e4-a14808192baf",
   "metadata": {},
   "source": [
    "## **Project 1: E-Commerce Price Tracker**  \n",
    "*Monitor product prices across major retailers and alert on price drops*\n",
    "\n",
    "### **Objective**\n",
    "Build a robust, maintainable scraper that:\n",
    "- Tracks prices of specific products on Amazon, Best Buy, and Walmart\n",
    "- Detects price changes over time\n",
    "- Sends email alerts when prices drop below a threshold\n",
    "- Handles anti-bot measures gracefully\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Project Setup & Dependencies**\n",
    "\n",
    "```python\n",
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 selenium pandas schedule smtplib python-dotenv lxml fake-useragent tenacity\n",
    "\n",
    "# Create project structure\n",
    "\"\"\"\n",
    "ecommerce_tracker/\n",
    "├── config/\n",
    "│   └── .env\n",
    "├── data/\n",
    "│   └── price_history.db\n",
    "├── scrapers/\n",
    "│   ├── __init__.py\n",
    "│   ├── amazon.py\n",
    "│   ├── bestbuy.py\n",
    "│   └── walmart.py\n",
    "├── utils/\n",
    "│   ├── database.py\n",
    "│   ├── email_alerts.py\n",
    "│   └── proxy_manager.py\n",
    "├── main.py\n",
    "└── requirements.txt\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### **Step 2: Configuration Management**\n",
    "\n",
    "```python\n",
    "# config/.env\n",
    "AMAZON_API_KEY=your_api_key_here\n",
    "EMAIL_HOST=smtp.gmail.com\n",
    "EMAIL_PORT=587\n",
    "EMAIL_USER=your_email@gmail.com\n",
    "EMAIL_PASS=your_app_password\n",
    "PROXY_LIST=http://proxy1:port,http://proxy2:port\n",
    "```\n",
    "\n",
    "```python\n",
    "# utils/config_loader.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    EMAIL_HOST = os.getenv('EMAIL_HOST')\n",
    "    EMAIL_PORT = int(os.getenv('EMAIL_PORT'))\n",
    "    EMAIL_USER = os.getenv('EMAIL_USER')\n",
    "    EMAIL_PASS = os.getenv('EMAIL_PASS')\n",
    "    PROXY_LIST = os.getenv('PROXY_LIST').split(',') if os.getenv('PROXY_LIST') else []\n",
    "```\n",
    "\n",
    "### **Step 3: Database Schema**\n",
    "\n",
    "```python\n",
    "# utils/database.py\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class PriceDatabase:\n",
    "    def __init__(self, db_path='data/price_history.db'):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS products (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                product_name TEXT NOT NULL,\n",
    "                retailer TEXT NOT NULL,\n",
    "                product_url TEXT UNIQUE NOT NULL,\n",
    "                target_price REAL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS price_history (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                product_id INTEGER,\n",
    "                current_price REAL,\n",
    "                currency TEXT DEFAULT 'USD',\n",
    "                in_stock BOOLEAN DEFAULT TRUE,\n",
    "                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (product_id) REFERENCES products (id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def add_product(self, product_name, retailer, product_url, target_price):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT OR IGNORE INTO products \n",
    "                (product_name, retailer, product_url, target_price)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            ''', (product_name, retailer, product_url, target_price))\n",
    "            conn.commit()\n",
    "            return cursor.lastrowid or self.get_product_id(product_url)\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def get_product_id(self, product_url):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT id FROM products WHERE product_url = ?', (product_url,))\n",
    "        result = cursor.fetchone()\n",
    "        conn.close()\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def save_price(self, product_id, price, currency='USD', in_stock=True):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            INSERT INTO price_history \n",
    "            (product_id, current_price, currency, in_stock)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (product_id, price, currency, in_stock))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def get_latest_price(self, product_id):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        query = '''\n",
    "            SELECT current_price, scraped_at \n",
    "            FROM price_history \n",
    "            WHERE product_id = ? \n",
    "            ORDER BY scraped_at DESC \n",
    "            LIMIT 1\n",
    "        '''\n",
    "        df = pd.read_sql_query(query, conn, params=(product_id,))\n",
    "        conn.close()\n",
    "        return df.iloc[0] if not df.empty else None\n",
    "    \n",
    "    def get_product_info(self, product_id):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        query = 'SELECT * FROM products WHERE id = ?'\n",
    "        df = pd.read_sql_query(query, conn, params=(product_id,))\n",
    "        conn.close()\n",
    "        return df.iloc[0] if not df.empty else None\n",
    "```\n",
    "\n",
    "### **Step 4: Base Scraper Class**\n",
    "\n",
    "```python\n",
    "# scrapers/base_scraper.py\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import time\n",
    "import random\n",
    "\n",
    "class BaseScraper:\n",
    "    def __init__(self, proxies=None):\n",
    "        self.session = requests.Session()\n",
    "        self.ua = UserAgent()\n",
    "        self.proxies = proxies or []\n",
    "        self.setup_session()\n",
    "    \n",
    "    def setup_session(self):\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': self.ua.random,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        })\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def make_request(self, url, use_proxy=True):\n",
    "        proxies = {}\n",
    "        if use_proxy and self.proxies:\n",
    "            proxy = random.choice(self.proxies)\n",
    "            proxies = {'http': proxy, 'https': proxy}\n",
    "        \n",
    "        response = self.session.get(\n",
    "            url, \n",
    "            timeout=10, \n",
    "            proxies=proxies if use_proxy else None\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    \n",
    "    def random_delay(self, min_sec=1, max_sec=3):\n",
    "        time.sleep(random.uniform(min_sec, max_sec))\n",
    "```\n",
    "\n",
    "### **Step 5: Amazon Scraper**\n",
    "\n",
    "```python\n",
    "# scrapers/amazon.py\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from .base_scraper import BaseScraper\n",
    "\n",
    "class AmazonScraper(BaseScraper):\n",
    "    def __init__(self, proxies=None):\n",
    "        super().__init__(proxies)\n",
    "        self.retailer = \"Amazon\"\n",
    "    \n",
    "    def extract_price(self, soup):\n",
    "        # Multiple possible selectors for price\n",
    "        price_selectors = [\n",
    "            '#corePriceDisplay_desktop_feature_div .a-price-whole',\n",
    "            '.a-price .a-offscreen',\n",
    "            '#priceblock_ourprice',\n",
    "            '.a-price.a-text-price .a-offscreen'\n",
    "        ]\n",
    "        \n",
    "        for selector in price_selectors:\n",
    "            price_elem = soup.select_one(selector)\n",
    "            if price_elem:\n",
    "                price_text = price_elem.get_text().strip()\n",
    "                # Extract numeric value\n",
    "                price_match = re.search(r'[\\d,]+\\.?\\d*', price_text.replace(',', ''))\n",
    "                if price_match:\n",
    "                    return float(price_match.group())\n",
    "        return None\n",
    "    \n",
    "    def is_in_stock(self, soup):\n",
    "        # Check for out of stock indicators\n",
    "        out_of_stock_selectors = [\n",
    "            '#availability .a-color-state',\n",
    "            '.a-color-error'\n",
    "        ]\n",
    "        \n",
    "        for selector in selector in out_of_stock_selectors:\n",
    "            elem = soup.select_one(selector)\n",
    "            if elem and 'unavailable' in elem.get_text().lower():\n",
    "                return False\n",
    "        \n",
    "        # Check add to cart button\n",
    "        add_to_cart = soup.select_one('#add-to-cart-button')\n",
    "        return add_to_cart is not None\n",
    "    \n",
    "    def scrape_product(self, product_url):\n",
    "        try:\n",
    "            response = self.make_request(product_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract product name\n",
    "            title_elem = soup.select_one('#productTitle')\n",
    "            product_name = title_elem.get_text().strip() if title_elem else \"Unknown Product\"\n",
    "            \n",
    "            # Extract price\n",
    "            price = self.extract_price(soup)\n",
    "            \n",
    "            # Check stock status\n",
    "            in_stock = self.is_in_stock(soup)\n",
    "            \n",
    "            return {\n",
    "                'product_name': product_name,\n",
    "                'price': price,\n",
    "                'in_stock': in_stock,\n",
    "                'currency': 'USD'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Amazon product {product_url}: {str(e)}\")\n",
    "            return None\n",
    "```\n",
    "\n",
    "### **Step 6: Best Buy Scraper**\n",
    "\n",
    "```python\n",
    "# scrapers/bestbuy.py\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from .base_scraper import BaseScraper\n",
    "\n",
    "class BestBuyScraper(BaseScraper):\n",
    "    def __init__(self, proxies=None):\n",
    "        super().__init__(proxies)\n",
    "        self.retailer = \"Best Buy\"\n",
    "    \n",
    "    def scrape_product(self, product_url):\n",
    "        try:\n",
    "            response = self.make_request(product_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Best Buy often has JSON-LD structured data\n",
    "            script_tag = soup.find('script', type='application/ld+json')\n",
    "            if script_tag:\n",
    "                try:\n",
    "                    data = json.loads(script_tag.string)\n",
    "                    if isinstance(data, list):\n",
    "                        data = data[0]\n",
    "                    \n",
    "                    product_name = data.get('name', 'Unknown Product')\n",
    "                    price = data.get('offers', {}).get('price')\n",
    "                    in_stock = data.get('offers', {}).get('availability', '') != 'OutOfStock'\n",
    "                    \n",
    "                    return {\n",
    "                        'product_name': product_name,\n",
    "                        'price': float(price) if price else None,\n",
    "                        'in_stock': in_stock,\n",
    "                        'currency': 'USD'\n",
    "                    }\n",
    "                except (json.JSONDecodeError, KeyError, TypeError):\n",
    "                    pass\n",
    "            \n",
    "            # Fallback to HTML parsing\n",
    "            price_elem = soup.select_one('.priceView-hero-price.priceView-customer-price span')\n",
    "            price = None\n",
    "            if price_elem:\n",
    "                price_text = price_elem.get_text()\n",
    "                import re\n",
    "                price_match = re.search(r'[\\d,]+\\.?\\d*', price_text.replace(',', ''))\n",
    "                price = float(price_match.group()) if price_match else None\n",
    "            \n",
    "            product_name_elem = soup.select_one('.sku-title h1')\n",
    "            product_name = product_name_elem.get_text().strip() if product_name_elem else \"Unknown Product\"\n",
    "            \n",
    "            # Check stock\n",
    "            add_to_cart = soup.select_one('[data-button-state=\"ADD_TO_CART\"]')\n",
    "            in_stock = add_to_cart is not None\n",
    "            \n",
    "            return {\n",
    "                'product_name': product_name,\n",
    "                'price': price,\n",
    "                'in_stock': in_stock,\n",
    "                'currency': 'USD'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Best Buy product {product_url}: {str(e)}\")\n",
    "            return None\n",
    "```\n",
    "\n",
    "### **Step 7: Email Alert System**\n",
    "\n",
    "```python\n",
    "# utils/email_alerts.py\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from .config_loader import Config\n",
    "\n",
    "class EmailAlert:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "    \n",
    "    def send_price_alert(self, product_info, current_price, previous_price, price_drop_percent):\n",
    "        subject = f\"Price Drop Alert: {product_info['product_name']}\"\n",
    "        body = f\"\"\"\n",
    "        Price Drop Detected!\n",
    "        \n",
    "        Product: {product_info['product_name']}\n",
    "        Retailer: {product_info['retailer']}\n",
    "        Current Price: ${current_price:.2f}\n",
    "        Previous Price: ${previous_price:.2f}\n",
    "        Price Drop: {price_drop_percent:.1f}%\n",
    "        Target Price: ${product_info['target_price']:.2f}\n",
    "        \n",
    "        View Product: {product_info['product_url']}\n",
    "        \"\"\"\n",
    "        \n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = self.config.EMAIL_USER\n",
    "        msg['To'] = self.config.EMAIL_USER\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "        \n",
    "        try:\n",
    "            server = smtplib.SMTP(self.config.EMAIL_HOST, self.config.EMAIL_PORT)\n",
    "            server.starttls()\n",
    "            server.login(self.config.EMAIL_USER, self.config.EMAIL_PASS)\n",
    "            text = msg.as_string()\n",
    "            server.sendmail(self.config.EMAIL_USER, self.config.EMAIL_USER, text)\n",
    "            server.quit()\n",
    "            print(f\"Email alert sent for {product_info['product_name']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to send email alert: {str(e)}\")\n",
    "```\n",
    "\n",
    "### **Step 8: Main Application Logic**\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "import time\n",
    "import schedule\n",
    "from scrapers.amazon import AmazonScraper\n",
    "from scrapers.bestbuy import BestBuyScraper\n",
    "from scrapers.walmart import WalmartScraper\n",
    "from utils.database import PriceDatabase\n",
    "from utils.email_alerts import EmailAlert\n",
    "from utils.config_loader import Config\n",
    "\n",
    "class PriceTracker:\n",
    "    def __init__(self):\n",
    "        self.db = PriceDatabase()\n",
    "        self.email_alert = EmailAlert()\n",
    "        self.config = Config()\n",
    "        self.scrapers = {\n",
    "            'Amazon': AmazonScraper(proxies=self.config.PROXY_LIST),\n",
    "            'Best Buy': BestBuyScraper(proxies=self.config.PROXY_LIST),\n",
    "            'Walmart': WalmartScraper(proxies=self.config.PROXY_LIST)\n",
    "        }\n",
    "    \n",
    "    def add_product_to_track(self, product_name, retailer, product_url, target_price):\n",
    "        product_id = self.db.add_product(product_name, retailer, product_url, target_price)\n",
    "        print(f\"Added product to track: {product_name} (ID: {product_id})\")\n",
    "        return product_id\n",
    "    \n",
    "    def check_single_product(self, product_id):\n",
    "        product_info = self.db.get_product_info(product_id)\n",
    "        if not product_info:\n",
    "            return\n",
    "        \n",
    "        retailer = product_info['retailer']\n",
    "        product_url = product_info['product_url']\n",
    "        \n",
    "        if retailer not in self.scrapers:\n",
    "            print(f\"Unsupported retailer: {retailer}\")\n",
    "            return\n",
    "        \n",
    "        scraper = self.scrapers[retailer]\n",
    "        product_data = scraper.scrape_product(product_url)\n",
    "        \n",
    "        if not product_data or product_data['price'] is None:\n",
    "            print(f\"Failed to scrape product: {product_url}\")\n",
    "            return\n",
    "        \n",
    "        # Save current price\n",
    "        self.db.save_price(\n",
    "            product_id, \n",
    "            product_data['price'], \n",
    "            product_data['currency'], \n",
    "            product_data['in_stock']\n",
    "        )\n",
    "        \n",
    "        # Check for price drop\n",
    "        latest_price_record = self.db.get_latest_price(product_id)\n",
    "        if latest_price_record is not None:\n",
    "            current_price = latest_price_record['current_price']\n",
    "            previous_price = self.get_previous_price(product_id, latest_price_record['scraped_at'])\n",
    "            \n",
    "            if previous_price and current_price < previous_price:\n",
    "                price_drop_percent = ((previous_price - current_price) / previous_price) * 100\n",
    "                target_price = product_info['target_price']\n",
    "                \n",
    "                # Send alert if price dropped below target or significant drop\n",
    "                if current_price <= target_price or price_drop_percent >= 5.0:\n",
    "                    self.email_alert.send_price_alert(\n",
    "                        product_info.to_dict(), \n",
    "                        current_price, \n",
    "                        previous_price, \n",
    "                        price_drop_percent\n",
    "                    )\n",
    "    \n",
    "    def get_previous_price(self, product_id, current_scraped_at):\n",
    "        conn = sqlite3.connect(self.db.db_path)\n",
    "        query = '''\n",
    "            SELECT current_price \n",
    "            FROM price_history \n",
    "            WHERE product_id = ? AND scraped_at < ?\n",
    "            ORDER BY scraped_at DESC \n",
    "            LIMIT 1\n",
    "        '''\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, (product_id, current_scraped_at))\n",
    "        result = cursor.fetchone()\n",
    "        conn.close()\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def run_full_check(self):\n",
    "        print(\"Starting full price check...\")\n",
    "        conn = sqlite3.connect(self.db.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT id FROM products')\n",
    "        product_ids = [row[0] for row in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        \n",
    "        for product_id in product_ids:\n",
    "            self.check_single_product(product_id)\n",
    "            # Be respectful - add delay between requests\n",
    "            time.sleep(2)\n",
    "        \n",
    "        print(\"Price check completed.\")\n",
    "    \n",
    "    def start_scheduler(self):\n",
    "        # Run every 6 hours\n",
    "        schedule.every(6).hours.do(self.run_full_check)\n",
    "        \n",
    "        # Also run immediately on startup\n",
    "        self.run_full_check()\n",
    "        \n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tracker = PriceTracker()\n",
    "    \n",
    "    # Add products to track\n",
    "    tracker.add_product_to_track(\n",
    "        \"Sony WH-1000XM4 Wireless Headphones\",\n",
    "        \"Amazon\",\n",
    "        \"https://www.amazon.com/dp/B0863TXGM3\",\n",
    "        250.00\n",
    "    )\n",
    "    \n",
    "    tracker.add_product_to_track(\n",
    "        \"Apple MacBook Air M2\",\n",
    "        \"Best Buy\",\n",
    "        \"https://www.bestbuy.com/site/apple-macbook-air-13-6-laptop-m2-chip-8gb-memory-256gb-ssd-midnight/6509650.p\",\n",
    "        900.00\n",
    "    )\n",
    "    \n",
    "    # Start the scheduler\n",
    "    tracker.start_scheduler()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ee1af-e4cd-4e7c-b6d2-c75ea7f068c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
