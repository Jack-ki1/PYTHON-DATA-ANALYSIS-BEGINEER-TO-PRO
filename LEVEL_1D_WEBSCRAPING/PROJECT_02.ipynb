{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80545978-fbf9-44ea-b41a-81c6800f9323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e0edf40-f6a0-410d-95e4-3d4427988ab6",
   "metadata": {},
   "source": [
    "## **Project 2: Job Board Aggregator**  \n",
    "*Scrape multiple job boards and create a unified job database with filtering capabilities*\n",
    "\n",
    "### **Objective**\n",
    "Build a comprehensive job aggregator that:\n",
    "- Scrapes Indeed, LinkedIn Jobs, and Glassdoor\n",
    "- Normalizes job data into a consistent format\n",
    "- Provides search and filtering capabilities\n",
    "- Updates job listings daily and removes expired posts\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Project Structure & Dependencies**\n",
    "\n",
    "```python\n",
    "# Install dependencies\n",
    "!pip install requests beautifulsoup4 selenium pandas flask apscheduler python-dateutil lxml\n",
    "\n",
    "# Project structure\n",
    "\"\"\"\n",
    "job_aggregator/\n",
    "├── scrapers/\n",
    "│   ├── indeed.py\n",
    "│   ├── linkedin.py\n",
    "│   └── glassdoor.py\n",
    "├── data/\n",
    "│   └── jobs.db\n",
    "├── api/\n",
    "│   └── app.py\n",
    "├── utils/\n",
    "│   ├── database.py\n",
    "│   ├── job_normalizer.py\n",
    "│   └── date_parser.py\n",
    "├── scheduler.py\n",
    "└── main.py\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### **Step 2: Database Design**\n",
    "\n",
    "```python\n",
    "# utils/database.py\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class JobDatabase:\n",
    "    def __init__(self, db_path='data/jobs.db'):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS jobs (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                job_title TEXT NOT NULL,\n",
    "                company_name TEXT NOT NULL,\n",
    "                location TEXT,\n",
    "                salary_min REAL,\n",
    "                salary_max REAL,\n",
    "                salary_currency TEXT DEFAULT 'USD',\n",
    "                job_url TEXT UNIQUE NOT NULL,\n",
    "                job_description TEXT,\n",
    "                job_type TEXT, -- full-time, part-time, contract, etc.\n",
    "                experience_level TEXT, -- entry, mid, senior\n",
    "                source TEXT NOT NULL, -- indeed, linkedin, glassdoor\n",
    "                posted_date DATE,\n",
    "                scraped_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT TRUE\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes for performance\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON jobs(source)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_location ON jobs(location)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_posted_date ON jobs(posted_date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_company ON jobs(company_name)')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def save_job(self, job_data):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO jobs \n",
    "                (job_title, company_name, location, salary_min, salary_max, \n",
    "                 salary_currency, job_url, job_description, job_type, \n",
    "                 experience_level, source, posted_date, is_active)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                job_data['job_title'],\n",
    "                job_data['company_name'],\n",
    "                job_data['location'],\n",
    "                job_data.get('salary_min'),\n",
    "                job_data.get('salary_max'),\n",
    "                job_data.get('salary_currency', 'USD'),\n",
    "                job_data['job_url'],\n",
    "                job_data.get('job_description', ''),\n",
    "                job_data.get('job_type'),\n",
    "                job_data.get('experience_level'),\n",
    "                job_data['source'],\n",
    "                job_data.get('posted_date'),\n",
    "                True\n",
    "            ))\n",
    "            conn.commit()\n",
    "            return cursor.lastrowid\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving job: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def get_jobs(self, filters=None, limit=100):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        base_query = \"SELECT * FROM jobs WHERE is_active = TRUE\"\n",
    "        params = []\n",
    "        \n",
    "        if filters:\n",
    "            if filters.get('location'):\n",
    "                base_query += \" AND location LIKE ?\"\n",
    "                params.append(f\"%{filters['location']}%\")\n",
    "            if filters.get('job_title'):\n",
    "                base_query += \" AND job_title LIKE ?\"\n",
    "                params.append(f\"%{filters['job_title']}%\")\n",
    "            if filters.get('company'):\n",
    "                base_query += \" AND company_name LIKE ?\"\n",
    "                params.append(f\"%{filters['company']}%\")\n",
    "            if filters.get('source'):\n",
    "                base_query += \" AND source = ?\"\n",
    "                params.append(filters['source'])\n",
    "            if filters.get('days_old'):\n",
    "                date_threshold = (datetime.now() - timedelta(days=filters['days_old'])).strftime('%Y-%m-%d')\n",
    "                base_query += \" AND posted_date >= ?\"\n",
    "                params.append(date_threshold)\n",
    "        \n",
    "        base_query += \" ORDER BY posted_date DESC LIMIT ?\"\n",
    "        params.append(limit)\n",
    "        \n",
    "        df = pd.read_sql_query(base_query, conn, params=params)\n",
    "        conn.close()\n",
    "        return df\n",
    "    \n",
    "    def mark_expired_jobs(self, days_old=30):\n",
    "        \"\"\"Mark jobs as inactive if older than specified days\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        date_threshold = (datetime.now() - timedelta(days=days_old)).strftime('%Y-%m-%d')\n",
    "        cursor.execute('''\n",
    "            UPDATE jobs \n",
    "            SET is_active = FALSE \n",
    "            WHERE posted_date < ? AND is_active = TRUE\n",
    "        ''', (date_threshold,))\n",
    "        affected_rows = cursor.rowcount\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return affected_rows\n",
    "```\n",
    "\n",
    "### **Step 3: Date Parser Utility**\n",
    "\n",
    "```python\n",
    "# utils/date_parser.py\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "class DateParser:\n",
    "    @staticmethod\n",
    "    def parse_posted_date(date_str, source):\n",
    "        \"\"\"\n",
    "        Parse various date formats from different job boards\n",
    "        \"\"\"\n",
    "        if not date_str:\n",
    "            return datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        date_str = date_str.lower().strip()\n",
    "        \n",
    "        # Handle relative dates (e.g., \"2 days ago\", \"Just posted\")\n",
    "        if 'ago' in date_str or 'just' in date_str or 'today' in date_str:\n",
    "            if 'hour' in date_str or 'min' in date_str or 'just' in date_str or 'today' in date_str:\n",
    "                return datetime.now().strftime('%Y-%m-%d')\n",
    "            elif 'day' in date_str:\n",
    "                match = re.search(r'(\\d+)', date_str)\n",
    "                if match:\n",
    "                    days_ago = int(match.group(1))\n",
    "                    date_obj = datetime.now() - timedelta(days=days_ago)\n",
    "                    return date_obj.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Handle absolute dates (MM/DD/YYYY, YYYY-MM-DD, etc.)\n",
    "        date_formats = [\n",
    "            '%Y-%m-%d',\n",
    "            '%m/%d/%Y',\n",
    "            '%d/%m/%Y',\n",
    "            '%B %d, %Y',\n",
    "            '%b %d, %Y',\n",
    "            '%d %B %Y',\n",
    "            '%d %b %Y'\n",
    "        ]\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                return date_obj.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Default to today if parsing fails\n",
    "        return datetime.now().strftime('%Y-%m-%d')\n",
    "```\n",
    "\n",
    "### **Step 4: Job Data Normalizer**\n",
    "\n",
    "```python\n",
    "# utils/job_normalizer.py\n",
    "import re\n",
    "\n",
    "class JobNormalizer:\n",
    "    @staticmethod\n",
    "    def normalize_salary(salary_text):\n",
    "        \"\"\"\n",
    "        Extract min and max salary from various formats\n",
    "        \"\"\"\n",
    "        if not salary_text:\n",
    "            return None, None\n",
    "        \n",
    "        salary_text = salary_text.replace(',', '').replace('$', '').lower()\n",
    "        \n",
    "        # Handle ranges like \"$50,000 - $70,000\"\n",
    "        range_pattern = r'([\\d.]+)\\s*(?:k|thousand)?\\s*[-–]\\s*([\\d.]+)\\s*(?:k|thousand)?'\n",
    "        match = re.search(range_pattern, salary_text)\n",
    "        if match:\n",
    "            min_val = float(match.group(1))\n",
    "            max_val = float(match.group(2))\n",
    "            # Handle 'k' notation\n",
    "            if 'k' in salary_text or 'thousand' in salary_text:\n",
    "                min_val *= 1000\n",
    "                max_val *= 1000\n",
    "            return min_val, max_val\n",
    "        \n",
    "        # Handle single values like \"$60,000/year\"\n",
    "        single_pattern = r'([\\d.]+)\\s*(?:k|thousand)?'\n",
    "        match = re.search(single_pattern, salary_text)\n",
    "        if match:\n",
    "            val = float(match.group(1))\n",
    "            if 'k' in salary_text or 'thousand' in salary_text:\n",
    "                val *= 1000\n",
    "            return val, val\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_job_type(job_type_text):\n",
    "        \"\"\"Normalize job type to standard values\"\"\"\n",
    "        if not job_type_text:\n",
    "            return None\n",
    "        \n",
    "        job_type_text = job_type_text.lower()\n",
    "        if any(word in job_type_text for word in ['full', 'full-time', 'full time']):\n",
    "            return 'full-time'\n",
    "        elif any(word in job_type_text for word in ['part', 'part-time', 'part time']):\n",
    "            return 'part-time'\n",
    "        elif any(word in job_type_text for word in ['contract', 'freelance']):\n",
    "            return 'contract'\n",
    "        elif 'intern' in job_type_text:\n",
    "            return 'internship'\n",
    "        else:\n",
    "            return 'full-time'  # Default assumption\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_experience_level(job_title):\n",
    "        \"\"\"Infer experience level from job title\"\"\"\n",
    "        if not job_title:\n",
    "            return None\n",
    "        \n",
    "        job_title = job_title.lower()\n",
    "        if any(word in job_title for word in ['senior', 'lead', 'principal', 'director', 'manager']):\n",
    "            return 'senior'\n",
    "        elif any(word in job_title for word in ['junior', 'associate', 'entry', 'graduate']):\n",
    "            return 'entry'\n",
    "        elif 'mid' in job_title or 'intermediate' in job_title:\n",
    "            return 'mid'\n",
    "        else:\n",
    "            return 'mid'  # Default assumption\n",
    "```\n",
    "\n",
    "### **Step 5: Indeed Scraper**\n",
    "\n",
    "```python\n",
    "# scrapers/indeed.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from ..utils.date_parser import DateParser\n",
    "from ..utils.job_normalizer import JobNormalizer\n",
    "\n",
    "class IndeedScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.indeed.com\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def search_jobs(self, query, location, max_pages=5):\n",
    "        jobs = []\n",
    "        for page in range(max_pages):\n",
    "            start = page * 10\n",
    "            search_url = f\"{self.base_url}/jobs?q={urllib.parse.quote(query)}&l={urllib.parse.quote(location)}&start={start}\"\n",
    "            \n",
    "            try:\n",
    "                response = self.session.get(search_url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                job_cards = soup.select('div.job_seen_beacon')\n",
    "                if not job_cards:\n",
    "                    break\n",
    "                \n",
    "                for card in job_cards:\n",
    "                    job_data = self.extract_job_data(card)\n",
    "                    if job_\n",
    "                        jobs.append(job_data)\n",
    "                \n",
    "                # Respect rate limits\n",
    "                import time\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping Indeed page {page}: {e}\")\n",
    "                break\n",
    "        \n",
    "        return jobs\n",
    "    \n",
    "    def extract_job_data(self, job_card):\n",
    "        try:\n",
    "            # Job title\n",
    "            title_elem = job_card.select_one('h2.jobTitle a')\n",
    "            if not title_elem:\n",
    "                return None\n",
    "            \n",
    "            job_title = title_elem.get('title') or title_elem.get_text().strip()\n",
    "            job_url = self.base_url + title_elem.get('href')\n",
    "            \n",
    "            # Company name\n",
    "            company_elem = job_card.select_one('[data-testid=\"company-name\"]')\n",
    "            company_name = company_elem.get_text().strip() if company_elem else \"Unknown Company\"\n",
    "            \n",
    "            # Location\n",
    "            location_elem = job_card.select_one('[data-testid=\"job-location\"]')\n",
    "            location = location_elem.get_text().strip() if location_elem else \"\"\n",
    "            \n",
    "            # Posted date\n",
    "            date_elem = job_card.select_one('span[data-testid=\"myJobsStateDate\"]')\n",
    "            posted_date_str = date_elem.get_text().strip() if date_elem else \"\"\n",
    "            posted_date = DateParser.parse_posted_date(posted_date_str, 'indeed')\n",
    "            \n",
    "            # Salary\n",
    "            salary_elem = job_card.select_one('[data-testid=\"attribute_snippet_testid\"]')\n",
    "            salary_text = salary_elem.get_text().strip() if salary_elem else \"\"\n",
    "            salary_min, salary_max = JobNormalizer.normalize_salary(salary_text)\n",
    "            \n",
    "            # Job type (from metadata if available)\n",
    "            job_type_elem = job_card.select_one('div.heading6.tapItem-gutter.metadataContainer')\n",
    "            job_type_text = job_type_elem.get_text().strip() if job_type_elem else \"\"\n",
    "            job_type = JobNormalizer.normalize_job_type(job_type_text)\n",
    "            \n",
    "            # Experience level\n",
    "            experience_level = JobNormalizer.normalize_experience_level(job_title)\n",
    "            \n",
    "            return {\n",
    "                'job_title': job_title,\n",
    "                'company_name': company_name,\n",
    "                'location': location,\n",
    "                'salary_min': salary_min,\n",
    "                'salary_max': salary_max,\n",
    "                'job_url': job_url,\n",
    "                'posted_date': posted_date,\n",
    "                'job_type': job_type,\n",
    "                'experience_level': experience_level,\n",
    "                'source': 'indeed'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job  {e}\")\n",
    "            return None\n",
    "```\n",
    "\n",
    "### **Step 6: LinkedIn Jobs Scraper (Using Selenium)**\n",
    "\n",
    "```python\n",
    "# scrapers/linkedin.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from ..utils.date_parser import DateParser\n",
    "from ..utils.job_normalizer import JobNormalizer\n",
    "\n",
    "class LinkedInScraper:\n",
    "    def __init__(self, headless=True):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.quit()\n",
    "    \n",
    "    def search_jobs(self, query, location, max_pages=3):\n",
    "        jobs = []\n",
    "        search_url = f\"https://www.linkedin.com/jobs/search/?keywords={quote(query)}&location={quote(location)}\"\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)  # Wait for initial load\n",
    "            \n",
    "            for page in range(max_pages):\n",
    "                # Scroll to load more jobs\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                job_listings = self.driver.find_elements(By.CSS_SELECTOR, \"div.base-card\")\n",
    "                \n",
    "                for listing in job_listings[len(jobs):]:  # Avoid duplicates\n",
    "                    try:\n",
    "                        job_data = self.extract_job_data(listing)\n",
    "                        if job_\n",
    "                            jobs.append(job_data)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                # Try to click next page\n",
    "                try:\n",
    "                    next_button = self.driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Next']\")\n",
    "                    if next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(3)\n",
    "                    else:\n",
    "                        break\n",
    "                except NoSuchElementException:\n",
    "                    break\n",
    "                \n",
    "                if len(jobs) >= 50:  # Limit total jobs\n",
    "                    break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping LinkedIn: {e}\")\n",
    "        \n",
    "        return jobs\n",
    "    \n",
    "    def extract_job_data(self, job_element):\n",
    "        try:\n",
    "            # Click to view job details\n",
    "            job_element.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Job title\n",
    "            try:\n",
    "                title_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h2.jobs-unified-top-card__job-title\")))\n",
    "                job_title = title_elem.text.strip()\n",
    "            except TimeoutException:\n",
    "                return None\n",
    "            \n",
    "            # Company name\n",
    "            try:\n",
    "                company_elem = self.driver.find_element(By.CSS_SELECTOR, \"span.jobs-unified-top-card__company-name a\")\n",
    "                company_name = company_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                company_name = \"Unknown Company\"\n",
    "            \n",
    "            # Location\n",
    "            try:\n",
    "                location_elem = self.driver.find_element(By.CSS_SELECTOR, \"span.jobs-unified-top-card__bullet\")\n",
    "                location = location_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                location = \"\"\n",
    "            \n",
    "            # Posted date\n",
    "            try:\n",
    "                date_elem = self.driver.find_element(By.CSS_SELECTOR, \"span.jobs-unified-top-card__posted-date\")\n",
    "                posted_date_str = date_elem.text.strip()\n",
    "                posted_date = DateParser.parse_posted_date(posted_date_str, 'linkedin')\n",
    "            except NoSuchElementException:\n",
    "                posted_date = DateParser.parse_posted_date(\"\", 'linkedin')\n",
    "            \n",
    "            # Get current URL for job posting\n",
    "            job_url = self.driver.current_url\n",
    "            \n",
    "            # Salary (LinkedIn rarely shows this publicly)\n",
    "            salary_min, salary_max = None, None\n",
    "            \n",
    "            # Job type and experience (inferred)\n",
    "            job_type = JobNormalizer.normalize_job_type(\"\")\n",
    "            experience_level = JobNormalizer.normalize_experience_level(job_title)\n",
    "            \n",
    "            return {\n",
    "                'job_title': job_title,\n",
    "                'company_name': company_name,\n",
    "                'location': location,\n",
    "                'salary_min': salary_min,\n",
    "                'salary_max': salary_max,\n",
    "                'job_url': job_url,\n",
    "                'posted_date': posted_date,\n",
    "                'job_type': job_type,\n",
    "                'experience_level': experience_level,\n",
    "                'source': 'linkedin'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return None\n",
    "```\n",
    "\n",
    "### **Step 7: Scheduler and Main Application**\n",
    "\n",
    "```python\n",
    "# scheduler.py\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from scrapers.indeed import IndeedScraper\n",
    "from scrapers.linkedin import LinkedInScraper\n",
    "from utils.database import JobDatabase\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class JobAggregatorScheduler:\n",
    "    def __init__(self):\n",
    "        self.db = JobDatabase()\n",
    "        self.search_queries = [\n",
    "            {'query': 'software engineer', 'location': 'Remote'},\n",
    "            {'query': 'data scientist', 'location': 'New York, NY'},\n",
    "            {'query': 'product manager', 'location': 'San Francisco, CA'}\n",
    "        ]\n",
    "    \n",
    "    def scrape_all_sources(self):\n",
    "        logger.info(\"Starting job aggregation...\")\n",
    "        \n",
    "        # Scrape Indeed\n",
    "        indeed_scraper = IndeedScraper()\n",
    "        for search in self.search_queries:\n",
    "            jobs = indeed_scraper.search_jobs(search['query'], search['location'])\n",
    "            for job in jobs:\n",
    "                self.db.save_job(job)\n",
    "            logger.info(f\"Scraped {len(jobs)} jobs from Indeed for {search['query']}\")\n",
    "        \n",
    "        # Scrape LinkedIn (more resource intensive)\n",
    "        linkedin_scraper = LinkedInScraper()\n",
    "        for search in self.search_queries[:1]:  # Limit LinkedIn due to restrictions\n",
    "            jobs = linkedin_scraper.search_jobs(search['query'], search['location'])\n",
    "            for job in jobs:\n",
    "                self.db.save_job(job)\n",
    "            logger.info(f\"Scraped {len(jobs)} jobs from LinkedIn for {search['query']}\")\n",
    "        \n",
    "        # Mark expired jobs\n",
    "        expired_count = self.db.mark_expired_jobs(days_old=30)\n",
    "        logger.info(f\"Marked {expired_count} jobs as expired\")\n",
    "        \n",
    "        logger.info(\"Job aggregation completed.\")\n",
    "    \n",
    "    def start_scheduler(self):\n",
    "        scheduler = BlockingScheduler()\n",
    "        # Run daily at 2 AM\n",
    "        scheduler.add_job(self.scrape_all_sources, 'cron', hour=2, minute=0)\n",
    "        scheduler.start()\n",
    "\n",
    "# main.py\n",
    "from scheduler import JobAggregatorScheduler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aggregator = JobAggregatorScheduler()\n",
    "    # Run once immediately for testing\n",
    "    aggregator.scrape_all_sources()\n",
    "    # Start scheduler for production\n",
    "    # aggregator.start_scheduler()\n",
    "```\n",
    "\n",
    "### **Step 8: Flask API for Job Search**\n",
    "\n",
    "```python\n",
    "# api/app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from utils.database import JobDatabase\n",
    "\n",
    "app = Flask(__name__)\n",
    "db = JobDatabase()\n",
    "\n",
    "@app.route('/api/jobs', methods=['GET'])\n",
    "def get_jobs():\n",
    "    filters = {}\n",
    "    \n",
    "    # Extract filters from query parameters\n",
    "    if request.args.get('location'):\n",
    "        filters['location'] = request.args.get('location')\n",
    "    if request.args.get('job_title'):\n",
    "        filters['job_title'] = request.args.get('job_title')\n",
    "    if request.args.get('company'):\n",
    "        filters['company'] = request.args.get('company')\n",
    "    if request.args.get('source'):\n",
    "        filters['source'] = request.args.get('source')\n",
    "    if request.args.get('days_old'):\n",
    "        filters['days_old'] = int(request.args.get('days_old'))\n",
    "    \n",
    "    limit = int(request.args.get('limit', 50))\n",
    "    jobs_df = db.get_jobs(filters=filters, limit=limit)\n",
    "    \n",
    "    # Convert to JSON\n",
    "    jobs_list = jobs_df.to_dict('records')\n",
    "    return jsonify({\n",
    "        'total': len(jobs_list),\n",
    "        'jobs': jobs_list\n",
    "    })\n",
    "\n",
    "@app.route('/api/stats', methods=['GET'])\n",
    "def get_stats():\n",
    "    conn = sqlite3.connect(db.db_path)\n",
    "    stats = {}\n",
    "    \n",
    "    # Total active jobs\n",
    "    stats['total_jobs'] = pd.read_sql_query(\"SELECT COUNT(*) as count FROM jobs WHERE is_active = TRUE\", conn).iloc[0]['count']\n",
    "    \n",
    "    # Jobs by source\n",
    "    stats['by_source'] = pd.read_sql_query(\"SELECT source, COUNT(*) as count FROM jobs WHERE is_active = TRUE GROUP BY source\", conn).to_dict('records')\n",
    "    \n",
    "    # Jobs by location (top 10)\n",
    "    stats['top_locations'] = pd.read_sql_query(\"SELECT location, COUNT(*) as count FROM jobs WHERE is_active = TRUE AND location != '' GROUP BY location ORDER BY count DESC LIMIT 10\", conn).to_dict('records')\n",
    "    \n",
    "    conn.close()\n",
    "    return jsonify(stats)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Deployment Considerations**\n",
    "\n",
    "### **For Production Deployment:**\n",
    "\n",
    "1. **Use Docker** for containerization\n",
    "2. **Implement proper error handling and logging**\n",
    "3. **Add rate limiting and proxy rotation**\n",
    "4. **Use environment variables for sensitive data**\n",
    "5. **Set up monitoring and alerts**\n",
    "6. **Consider using Scrapy for large-scale scraping**\n",
    "7. **Implement data validation and cleaning pipelines**\n",
    "\n",
    "### **Ethical and Legal Compliance:**\n",
    "- Always respect `robots.txt`\n",
    "- Implement reasonable delays between requests\n",
    "- Don't scrape personal information\n",
    "- Check terms of service for each website\n",
    "- Consider using official APIs when available\n",
    "\n",
    "These projects demonstrate professional-grade web scraping implementations that handle real-world complexities including anti-bot measures, data normalization, scheduling, and user interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ee1af-e4cd-4e7c-b6d2-c75ea7f068c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
