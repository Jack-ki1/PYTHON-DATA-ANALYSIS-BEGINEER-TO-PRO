{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdf5a0-0415-4c72-98ca-e425a0093006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e08b47-7ea4-4072-bfe0-39cabbcc1805",
   "metadata": {},
   "source": [
    "# **Comprehensive Course on Web Scraping in Python**  \n",
    "*From Beginner to Professional*\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction to Web Scraping](#1-introduction-to-web-scraping)  \n",
    "2. [Setting Up Your Environment](#2-setting-up-your-environment)  \n",
    "3. [Understanding HTML and the DOM](#3-understanding-html-and-the-dom)  \n",
    "4. [HTTP Basics for Web Scrapers](#4-http-basics-for-web-scrapers)  \n",
    "5. [Parsing HTML with BeautifulSoup](#5-parsing-html-with-beautifulsoup)  \n",
    "6. [Advanced HTML Parsing Techniques](#6-advanced-html-parsing-techniques)  \n",
    "7. [Working with APIs and JSON Data](#7-working-with-apis-and-json-data)  \n",
    "8. [Handling Dynamic Content with Selenium](#8-handling-dynamic-content-with-selenium)  \n",
    "9. [Dealing with JavaScript-Heavy Websites](#9-dealing-with-javascript-heavy-websites)  \n",
    "10. [Managing Sessions, Cookies, and Authentication](#10-managing-sessions-cookies-and-authentication)  \n",
    "11. [Respecting `robots.txt` and Ethical Scraping](#11-respecting-robotstxt-and-ethical-scraping)  \n",
    "12. [Avoiding Blocks: Headers, Proxies, and Delays](#12-avoiding-blocks-headers-proxies-and-delays)  \n",
    "13. [Scraping at Scale with Concurrency](#13-scraping-at-scale-with-concurrency)  \n",
    "14. [Storing and Structuring Scraped Data](#14-storing-and-structuring-scraped-data)  \n",
    "15. [Error Handling and Robust Scrapers](#15-error-handling-and-robust-scrapers)  \n",
    "16. [Legal and Ethical Considerations](#16-legal-and-ethical-considerations)  \n",
    "17. [Case Studies](#17-case-studies)  \n",
    "18. [Best Practices and Final Tips](#18-best-practices-and-final-tips)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fea0d-81cc-42a7-8a4f-c97e3c4acdba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1. Introduction to Web Scraping**\n",
    "\n",
    "Web scraping is the automated process of extracting data from websites. It involves fetching web pages, parsing their content, and transforming unstructured HTML into structured data (e.g., CSV, JSON, databases).\n",
    "\n",
    "### Why Learn Web Scraping?\n",
    "- **Data Collection**: Gather data for research, analysis, or machine learning.\n",
    "- **Price Monitoring**: Track competitor pricing.\n",
    "- **Content Aggregation**: Build news feeds or job boards.\n",
    "- **Automation**: Replace manual copy-paste workflows.\n",
    "\n",
    "### What You’ll Learn\n",
    "By the end of this course, you will:\n",
    "- Scrape static and dynamic websites.\n",
    "- Handle authentication, sessions, and anti-bot measures.\n",
    "- Build scalable, ethical, and maintainable scrapers.\n",
    "- Store data efficiently and avoid common pitfalls.\n",
    "\n",
    "> **Note**: Always check a website’s `robots.txt` and terms of service before scraping.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355ac43-bc83-4513-801f-168613b5b48f",
   "metadata": {},
   "source": [
    "## **2. Setting Up Your Environment**\n",
    "\n",
    "We’ll use Python 3.8+ and essential libraries.\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4 lxml selenium pandas numpy scrapy fake-useragent\n",
    "```\n",
    "\n",
    "For Selenium, also install a WebDriver (e.g., ChromeDriver):\n",
    "- Download from [ChromeDriver](https://chromedriver.chromium.org/)\n",
    "- Place it in your system PATH or specify its path in code.\n",
    "\n",
    "### Verify Installation\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Environment ready!\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5b9d4-e825-45dd-acb2-daab869beb2d",
   "metadata": {},
   "source": [
    "## **3. Understanding HTML and the DOM**\n",
    "\n",
    "Web scraping relies on understanding HTML structure.\n",
    "\n",
    "### Basic HTML Structure\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Main Heading</h1>\n",
    "    <p class=\"intro\">This is a paragraph.</p>\n",
    "    <ul id=\"menu\">\n",
    "        <li><a href=\"/home\">Home</a></li>\n",
    "        <li><a href=\"/about\">About</a></li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c5c66-d56d-41b6-b38c-e96fb6c81933",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "- **Tags**: `<h1>`, `<p>`, `<a>`, etc.\n",
    "- **Attributes**: `class`, `id`, `href`.\n",
    "- **DOM (Document Object Model)**: Tree representation of HTML.\n",
    "\n",
    "### Inspecting Pages\n",
    "Use browser DevTools (`Ctrl+Shift+I` or `Cmd+Option+I`) to:\n",
    "- Inspect elements.\n",
    "- Copy selectors (CSS or XPath).\n",
    "- Monitor network requests.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94f4ef-41e9-46d7-b7bf-0c760af1ac83",
   "metadata": {},
   "source": [
    "## **4. HTTP Basics for Web Scrapers**\n",
    "\n",
    "Scrapers communicate via HTTP requests.\n",
    "\n",
    "### Common HTTP Methods\n",
    "- **GET**: Retrieve data (most common for scraping).\n",
    "- **POST**: Submit data (e.g., login forms).\n",
    "\n",
    "### HTTP Status Codes\n",
    "- `200`: OK\n",
    "- `403`: Forbidden\n",
    "- `404`: Not Found\n",
    "- `500`: Server Error\n",
    "\n",
    "### Making Requests with `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a44bcd96-2754-4b39-8887-eaab9b38abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Headers: {'Date': 'Wed, 04 Feb 2026 09:52:30 GMT', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'last-modified': 'Wed, 04 Feb 2026 05:22:19 GMT', 'allow': 'GET, HEAD', 'Age': '9304', 'cf-cache-status': 'HIT', 'Vary': 'Accept-Encoding', 'Server': 'cloudflare', 'CF-RAY': '9c89468d4fb6d499-FCO'}\n",
      "Content Type: text/html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Headers: {response.headers}\")\n",
    "print(f\"Content Type: {response.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5d315-9106-4a78-9753-5847641f395c",
   "metadata": {},
   "source": [
    "> **Tip**: Always check `response.status_code` before parsing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a2ab2-d2df-4cfa-ae5e-3f51d38761ae",
   "metadata": {},
   "source": [
    "## **5. Parsing HTML with BeautifulSoup**\n",
    "\n",
    "BeautifulSoup converts HTML into a parse tree for easy navigation.\n",
    "\n",
    "### Basic Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3115e758-8cba-4d35-8adb-0222dac573ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<h1>Title</h1>\n",
    "<p class=\"text\">Hello World</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup.h1.text)  # Output: Title\n",
    "print(soup.find('p', class_='text').text)  # Output: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae688cb-f850-4f18-833d-2fcfb7fd5c22",
   "metadata": {},
   "source": [
    "### Key Methods\n",
    "- `.find()`: First matching element.\n",
    "- `.find_all()`: All matching elements.\n",
    "- `.select()`: CSS selector syntax.\n",
    "\n",
    "### Example: Scrape Quotes from quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d3ca16-4720-4073-bd79-7702483b0612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\" - Albert Einstein\n",
      "\"“It is our choices, Harry, that show what we truly are, far more than our abilities.”\" - J.K. Rowling\n",
      "\"“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\" - Albert Einstein\n",
      "\"“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\" - Jane Austen\n",
      "\"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\" - Marilyn Monroe\n",
      "\"“Try not to become a man of success. Rather become a man of value.”\" - Albert Einstein\n",
      "\"“It is better to be hated for what you are than to be loved for what you are not.”\" - André Gide\n",
      "\"“I have not failed. I've just found 10,000 ways that won't work.”\" - Thomas A. Edison\n",
      "\"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\" - Eleanor Roosevelt\n",
      "\"“A day without sunshine is like, you know, night.”\" - Steve Martin\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "quotes = soup.find_all('div', class_='quote')\n",
    "for quote in quotes:\n",
    "    text = quote.find('span', class_='text').text\n",
    "    author = quote.find('small', class_='author').text\n",
    "    print(f'\"{text}\" - {author}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367bdc7-71ee-430d-8b36-41e7998e25e4",
   "metadata": {},
   "source": [
    "## **6. Advanced HTML Parsing Techniques**\n",
    "\n",
    "### Navigating the Parse Tree\n",
    "- `.parent`, `.children`, `.next_sibling`, `.previous_sibling`\n",
    "\n",
    "### Using CSS Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc1f4709-966d-42ab-a5db-30da63f47ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all links inside <div class=\"menu\">\n",
    "links = soup.select('div.menu a')\n",
    "\n",
    "# Select by ID\n",
    "header = soup.select_one('#main-header')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d77f1b-6856-40bb-9d07-7351211bdc8d",
   "metadata": {},
   "source": [
    "### Extracting Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb809302-5984-4263-80c4-eeadb7c3827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = soup.find('a')\n",
    "href = link['href']  # or link.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7c66b-54d2-48db-952a-ca2addeefd46",
   "metadata": {},
   "source": [
    "### Handling Malformed HTML\n",
    "Use `lxml` parser for speed and robustness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78943b76-5d71-45ce-a0c8-ae38873f8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b04e06-5105-4f5b-9e25-738d098c6708",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **7. Working with APIs and JSON Data**\n",
    "\n",
    "Many sites load data via AJAX calls to APIs.\n",
    "\n",
    "### Inspecting Network Requests\n",
    "In DevTools > Network tab:\n",
    "- Look for XHR/fetch requests.\n",
    "- Identify JSON endpoints.\n",
    "\n",
    "### Fetching JSON Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95738e-7908-4155-8d23-a11702561771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_url = \"https://api.example.com/data\"\n",
    "response = requests.get(api_url)\n",
    "data = response.json()  # Parse JSON\n",
    "\n",
    "for item in data['items']:\n",
    "    print(item['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b009759-23db-485e-a6ca-54ed27218ad7",
   "metadata": {},
   "source": [
    "> **Advantage**: Faster and more reliable than scraping HTML.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d29dd-85b8-4feb-b041-f20f9dfa6527",
   "metadata": {},
   "source": [
    "## **8. Handling Dynamic Content with Selenium**\n",
    "\n",
    "Some content loads via JavaScript after page load.\n",
    "\n",
    "### When to Use Selenium\n",
    "- Content appears after user interaction (clicks, scrolls).\n",
    "- Page uses heavy JavaScript (React, Angular, Vue).\n",
    "\n",
    "### Basic Selenium Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58b213-da5e-4956-a0a0-ca28edb8d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://example.com\")\n",
    "\n",
    "# Wait for element to load\n",
    "element = driver.find_element(By.CLASS_NAME, \"dynamic-content\")\n",
    "print(element.text)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fb28e-1386-4686-9898-ba08c683c498",
   "metadata": {},
   "source": [
    "### Waits (Critical!)\n",
    "- **Implicit Wait**: `driver.implicitly_wait(10)`\n",
    "- **Explicit Wait**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f298f-2b1c-44f2-879e-0ccb526c9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "element = wait.until(EC.presence_of_element_located((By.ID, \"myId\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c8374-a963-4c21-bb76-0eb0dd2d5032",
   "metadata": {},
   "source": [
    "## **9. Dealing with JavaScript-Heavy Websites**\n",
    "\n",
    "### Common Patterns\n",
    "- Infinite scroll\n",
    "- Lazy-loaded images\n",
    "- Single-page applications (SPAs)\n",
    "\n",
    "### Example: Scrolling to Load More Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312df95-c831-4627-9a58-998559b2420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)  # Wait for content to load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a35b9-cd10-42ce-b397-8bf9545e16fc",
   "metadata": {},
   "source": [
    "### Extracting Data Rendered by JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4815c-0407-4d97-8a62-8e6eb4841158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After JS execution, get page source\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4abd9-c9a6-405f-84d6-dd5f168668a8",
   "metadata": {},
   "source": [
    "## **10. Managing Sessions, Cookies, and Authentication**\n",
    "\n",
    "### Session Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430ee67-3300-4945-8618-3a9208c87276",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.get(\"https://example.com/login\")  # Sets cookies\n",
    "response = session.post(\"https://example.com/login\", data=login_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062c14b-5fc1-4de7-b798-3a026ba8a421",
   "metadata": {},
   "source": [
    "### Logging In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ab5dc-6b16-433a-a4fa-bdddb79183dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "login_data = {\n",
    "    'username': 'user',\n",
    "    'password': 'pass'\n",
    "}\n",
    "session.post(login_url, data=login_data)\n",
    "# Now session has auth cookies\n",
    "protected_page = session.get(protected_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419a1d1-6690-4775-8a03-861a3315a0e7",
   "metadata": {},
   "source": [
    "### Handling CSRF Tokens\n",
    "\n",
    "#### Fetch login page to get token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f928a-cfa5-459e-821f-e34e9029e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_page = session.get(login_url)\n",
    "soup = BeautifulSoup(login_page.text, 'html.parser')\n",
    "csrf_token = soup.find('input', {'name': 'csrf_token'})['value']\n",
    "\n",
    "login_data['csrf_token'] = csrf_token\n",
    "session.post(login_url, data=login_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734366c-a198-476f-ac70-c486be96c07d",
   "metadata": {},
   "source": [
    "## **11. Respecting `robots.txt` and Ethical Scraping**\n",
    "\n",
    "### What is `robots.txt`?\n",
    "A file at `https://example.com/robots.txt` that specifies scraping rules.\n",
    "\n",
    "Example:\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /admin/\n",
    "Crawl-delay: 10\n",
    "```\n",
    "\n",
    "### Check `robots.txt` Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17f61f-503b-4646-a550-ad096db14180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url(\"https://example.com/robots.txt\")\n",
    "rp.read()\n",
    "\n",
    "can_scrape = rp.can_fetch('*', 'https://example.com/data')\n",
    "print(f\"Allowed to scrape: {can_scrape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d339b1-f1c6-41b0-b120-9cf96341430c",
   "metadata": {},
   "source": [
    "### Ethical Guidelines\n",
    "- **Rate Limiting**: Add delays between requests.\n",
    "- **Identify Yourself**: Use a descriptive `User-Agent`.\n",
    "- **Don’t Overload Servers**: Scrape during off-peak hours.\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Avoiding Blocks: Headers, Proxies, and Delays**\n",
    "\n",
    "### Custom Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74871210-beb7-451d-88f5-ce87d52fbbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e66d05-fdcb-4be5-b0e8-286037e283b0",
   "metadata": {},
   "source": [
    "### Rotating User-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd3e47-add6-41af-8b2b-33a8c5b0ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "headers = {'User-Agent': ua.random}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e3d90-299e-400e-a3a5-2197cc20cd48",
   "metadata": {},
   "source": [
    "### Using Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fad13-10e8-4e56-aad5-e162da66e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = {\n",
    "    'http': 'http://10.10.1.10:3128',\n",
    "    'https': 'http://10.10.1.10:1080',\n",
    "}\n",
    "response = requests.get(url, proxies=proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b542e-1a24-4158-bddb-8c3c26a0cee2",
   "metadata": {},
   "source": [
    "> **Note**: Free proxies are unreliable. Use paid services for production.\n",
    "\n",
    "### Adding Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b7047-0f70-49b1-92f6-a695195b942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "time.sleep(random.uniform(1, 3))  # Sleep 1-3 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95647c65-bd04-470a-82d3-b5664e3595e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **13. Scraping at Scale with Concurrency**\n",
    "\n",
    "### Threading vs Async\n",
    "\n",
    "- **Threading**: Good for I/O-bound tasks (network requests).\n",
    "- **Async/Await**: More efficient for high concurrency.\n",
    "\n",
    "### Example with `concurrent.futures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24e859-1f3e-4995-a3c9-3b7d936b7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def fetch(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "urls = [\"https://example.com/page1\", ...]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = executor.map(fetch, urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c4f87-1029-49b0-8c7f-68efe8a0d2e0",
   "metadata": {},
   "source": [
    "### Async with `aiohttp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd246c90-bc9e-4146-b11c-dd5cbf703ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch(session, url) for url in urls]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        return responses\n",
    "\n",
    "results = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0588a39-087e-44eb-be85-96266d5dc9cc",
   "metadata": {},
   "source": [
    "## **14. Storing and Structuring Scraped Data**\n",
    "\n",
    "### Saving to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c292c-596c-4840-abe2-3153a3db488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = [{'name': 'Alice', 'age': 30}, {'name': 'Bob', 'age': 25}]\n",
    "\n",
    "with open('data.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['name', 'age'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a2163-8c45-448c-8819-6acfed7d897c",
   "metadata": {},
   "source": [
    "### Saving to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11665ae6-887b-4c48-9e33-a06af521c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547a9cc-f546-47ac-ab15-bc94a167ac86",
   "metadata": {},
   "source": [
    "### Saving to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f711b4-2fae-45f6-9152-5ebc62f3cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('data.csv', index=False)\n",
    "df.to_json('data.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c9c54-0ff0-4f61-9fa8-d613f4dbf3ea",
   "metadata": {},
   "source": [
    "### Databases (SQLite Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d05adc-11c7-4718-b252-c024687b7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('scraped_data.db')\n",
    "df.to_sql('users', conn, if_exists='replace', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b44a98-fda4-48f6-a6bc-9219d077f41c",
   "metadata": {},
   "source": [
    "## **15. Error Handling and Robust Scrapers**\n",
    "\n",
    "### Common Errors\n",
    "- `ConnectionError`: Network issues.\n",
    "- `Timeout`: Server too slow.\n",
    "- `AttributeError`: Element not found.\n",
    "\n",
    "### Try-Except Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7392e0-c08f-4895-a2a3-a1d3a023bf8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()  # Raises HTTPError for bad status\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c06ba8-0608-4000-afdc-69da663acfa0",
   "metadata": {},
   "source": [
    "### Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47462a60-d530-49d4-a3a7-34cdc7c4cbf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def retry(max_attempts=3, delay=1):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_attempts - 1:\n",
    "                        raise e\n",
    "                    time.sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry(max_attempts=3, delay=2)\n",
    "def scrape_page(url):\n",
    "    return requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac727f-4e36-4f98-8b7b-f18c8545c630",
   "metadata": {},
   "source": [
    "## **16. Legal and Ethical Considerations**\n",
    "\n",
    "### Key Points\n",
    "- **Copyright**: Scraped content may be protected.\n",
    "- **Terms of Service**: Violating ToS can lead to legal action.\n",
    "- **Personal Data**: GDPR/CCPA compliance required.\n",
    "\n",
    "### Best Practices\n",
    "- **Public Data Only**: Avoid private or sensitive info.\n",
    "- **Attribution**: Credit original sources.\n",
    "- **Consult Legal Counsel**: For commercial projects.\n",
    "\n",
    "> **Disclaimer**: This course does not constitute legal advice.\n",
    "\n",
    "---\n",
    "\n",
    "## **17. Case Studies**\n",
    "\n",
    "### Case Study 1: E-commerce Price Tracker\n",
    "- **Goal**: Monitor product prices on Amazon.\n",
    "- **Challenges**: Dynamic content, anti-bot measures.\n",
    "- **Solution**: \n",
    "  - Use Selenium for JS rendering.\n",
    "  - Rotate proxies and user-agents.\n",
    "  - Store price history in a database.\n",
    "\n",
    "### Case Study 2: News Aggregator\n",
    "- **Goal**: Collect headlines from multiple sources.\n",
    "- **Challenges**: Different HTML structures.\n",
    "- **Solution**:\n",
    "  - Create modular parsers per site.\n",
    "  - Use RSS feeds when available.\n",
    "  - Schedule daily runs with cron.\n",
    "\n",
    "### Case Study 3: Real Estate Listings\n",
    "- **Goal**: Scrape property details from Zillow.\n",
    "- **Challenges**: Pagination, CAPTCHAs.\n",
    "- **Solution**:\n",
    "  - Respect `robots.txt`.\n",
    "  - Implement human-like delays.\n",
    "  - Use headless browsers cautiously.\n",
    "\n",
    "---\n",
    "\n",
    "## **18. Best Practices and Final Tips**\n",
    "\n",
    "### Do’s and Don’ts\n",
    "| Do | Don’t |\n",
    "|----|-------|\n",
    "| Check `robots.txt` | Ignore rate limits |\n",
    "| Use descriptive User-Agents | Scrape personal data |\n",
    "| Handle errors gracefully | Hardcode selectors |\n",
    "| Store data responsibly | Assume structure won’t change |\n",
    "\n",
    "### Maintaining Scrapers\n",
    "- **Modularize Code**: Separate fetching, parsing, and storage.\n",
    "- **Monitor Changes**: Websites update frequently—set up alerts.\n",
    "- **Log Everything**: Debugging is easier with logs.\n",
    "\n",
    "### Final Project Idea\n",
    "Build a scraper that:\n",
    "1. Logs into a site (e.g., GitHub).\n",
    "2. Scrapes user repositories.\n",
    "3. Stores data in a SQLite DB.\n",
    "4. Runs daily via a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3345b-907a-44f2-a076-6a388230437d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "You now possess the knowledge to build professional-grade web scrapers—from simple static pages to complex, dynamic applications. Remember:\n",
    "\n",
    "> **\"With great power comes great responsibility.\"**\n",
    "\n",
    "Always scrape ethically, legally, and sustainably. Happy scraping!\n",
    "\n",
    "---\n",
    "\n",
    "*End of Course*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fda51-eb3e-4761-8305-514c5c568825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
