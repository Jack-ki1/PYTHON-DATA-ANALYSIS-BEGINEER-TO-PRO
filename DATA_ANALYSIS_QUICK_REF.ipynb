{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e7746f-0f67-4239-9d99-c6959d243bb2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:darkblue; padding:40px; text-align:center; border-radius:10px;\">\n",
    "\n",
    "  <h1 style=\"color:pink; font-size:3em; margin-bottom:20px;\">WELCOME TO MY PYTHON COMMANDS REFERENCE</h1>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076ae22-fdc4-4689-bb7d-37a69c70e402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51873d7f-1ff9-4d02-b9c7-f957cca3cf77",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\"><h4>HTML CODE APPLIED FOR STYLING YOUR WORK</h4></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf090cd8-2f5f-44c5-a04c-5ee08ffdec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "df=pd.read_csv()\n",
    "df=pd.read_excel(\"TIMETABLE_WORK_DATA.xlsx\", engine=\"openpyxl\")\n",
    "df[:4]\n",
    "\n",
    "#UNDERSTANDING YOUR DATA\n",
    "df.head()\n",
    "df.tail()\n",
    "print(\"shape of dataframe is:\",df.shape)\n",
    "df.describe()\n",
    "df.info()\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737eac8-7295-44f6-b240-4126fd629372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[col.upper() for col in df]#.lower() for lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cee660-5b06-475b-b5d5-67adfa920d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566aa95b-3127-45a4-ba24-49386d34f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=50\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a8a26-79a8-4d30-85ff-987fc964d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking DUPLICATES Values\n",
    "df.duplicated(): Returns a Boolean Series forduplicate rows\n",
    "df[df.duplicated()]: Displays duplicate rows\n",
    "df.drop_duplicates(): Removes duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234eb68-468e-4ea0-bb81-a21598ca7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display nulls\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4aeb4-e7ee-4ac9-bbbd-4ef1a81de6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'A' and 'C' columns\n",
    "df.drop(['A', 'C'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334450e9-3632-454b-b2a0-cdc72843ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()\n",
    "\n",
    "df=df.dropna(subset='col_name')\n",
    "\n",
    "df1.dropna(axis='columns', how='all')\n",
    "\n",
    "df2.dropna(axis='rows', thresh=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6388a-fdf6-4b1d-b193-dd09d477a182",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.fillna(value):   Replaces missing values with a specified value\n",
    " df.fillna(df.median()):   Fills missing values withmedian\n",
    "df['column_name'].fillna(method='ffill')\n",
    "df['column_name'].fillna(method='bfill')\n",
    "\n",
    "df[\"\"] = df[\"\"].fillna(df[\"\"].mean())\n",
    "# Fill missing values in the 'Product_Price' column with its mean\n",
    "df.loc[:, 'Product_Price'] = df['Product_Price'].fillna(df['Product_Price'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3952306a-7ad2-4113-a7be-dc9993cec348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object'])[:5] #can be: 'number'\n",
    "df.select_dtypes(include=['object'], exclude=['int64','float64'])#parameters are include or exclude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65473e23-537d-4cd6-a9d2-9802fe709c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"column\"].value_counts(): Counts occurrences of each unique value\n",
    "df[\"column\"].value_counts(normalize=True): Normalized value counts (percentage)\n",
    "df[\"column\"].unique(): Lists unique values\n",
    "df[\"column\"].nunique(): Number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39d2bd-1e3e-4c69-950b-2e88b7bd4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(): Correlation matrix (Pearson by default)\n",
    "df.corr(method=\"kendall\"): Kendall correlation\n",
    "df.corr(method=\"spearman\"): Spearman correlation\n",
    "df.cov(): Covariance matrix\n",
    "\n",
    "df['Total_Spent'].corr(df['Price_per_Unit']).round(2)\n",
    "\n",
    "df['Total_Spent'].cov(df['Price_per_Unit']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0f321-8d6d-4c9f-a8e9-46ffbc89b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Spent'].abs()\n",
    "df['Total_Spent'].skew().round(3): calculate skewness\n",
    "df['Total_Spent'].kurt().round(3): calculate kurtosis\n",
    "df['Total_Spent'].idxmin() \n",
    "df['Total_Spent'].idxmax()\n",
    "df.loc[df[\"Area in Square Km.\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0095a6-f3b1-4119-9dbe-9ac980f04ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"column\"].str.lower(): Converts text to lowercase\n",
    "df[\"column\"].str.upper(): Converts text to uppercase\n",
    "df[\"column\"].str.strip(): Removes leading/trailing spaces\n",
    "df['column1'].str.replace('Credit Card', 'BANK_CARD'): Replaces text\n",
    " df[\"column\"].astype(\"int\"): Converts column to integer type\n",
    "\n",
    "# Convert the column to string and handle non-string values\n",
    "df.loc[:, 'column_name'] = df['same_column_name'].astype(str).str.replace('%', '').astype(int)\n",
    "\n",
    "#splitting a column\n",
    "#[0]are the elements in column\n",
    "df['Phone_model']= df['Phone Title'].str.split(' ', expand=True)[0]\n",
    "\n",
    "\n",
    "df['Storage'] = df['Specs'].str.split(',').apply(lambda x: ' '.join(x[4:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5f2b4-6921-4057-9c8e-754c122c50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a column from string encoded values like \"477.9M\" to an integer one having value such as = 477900000\n",
    "# Clean the data first\n",
    "df[\"SUBSCRIBERS\"] = (df[\"SUBSCRIBERS\"].astype(str).str.replace(\"'\", \"\").str.replace(\"'\", \"\").str.strip().replace({'M': 'e6'}, regex=True)\n",
    "    .astype(float)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "#0r\n",
    "# Clean the data first\n",
    "df[\"SUBSCRIBERS\"] = (df[\"SUBSCRIBERS\"].astype(str).str.replace(\"'\", \"\").str.replace(\".\", \"\").str.replace(\"M\", \"\"))\n",
    "\n",
    "# Convert to float first, then multiply, then to integer\n",
    "df[\"SUBSCRIBERS\"] = (df[\"SUBSCRIBERS\"].astype(int) * 1_000_00)\n",
    "\n",
    "#for thousands and hunderds of thousands values use \n",
    "df[\"AUTHENTIC_ENGAGEMENT_NOW\"] = (df[\"AUTHENTIC_ENGAGEMENT_NOW\"].astype(str).str.replace(\"'\", \"\").str.replace(\"'\", \"\").str.strip()\n",
    "    .replace({'K': 'e3', 'M': 'e6'}, regex=True)\n",
    "    .astype(float)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815a55c-64bf-464d-bf78-72aed7b15804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Spent'].agg(['count','mean','median','min','max', 'sum','std', 'var'])\n",
    "\n",
    "df2.agg({'Total_Spent' : ['sum', 'min'], 'Quantity' : ['min', 'max']})\n",
    "#other: cumsum(), cumprod(), diff(), pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363c224-909c-4f70-8cea-7945530c7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"column\"] =pd.to_datetime(df[\"column\"]): Converts column to datetime\n",
    "# Ensure Order_Date is datetime format\n",
    "df[\"date_column\"] = pd.to_datetime(df[\"date_column\"], format=\"%m/%d/%Y %H:%M\")\n",
    "df[\"date_column\"] = pd.to_datetime(df[\"date_column\"], format=\"mixed\")\n",
    "\n",
    "# Example: Change to 'DD-MM-YYYY' format\n",
    "df['Formatted_Date'] = df['Date'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "#Example: Change to 'MM/DD/YYYY HH:MM:SS' format\n",
    "df['Full_Formatted_Date'] = df['Date'].dt.strftime('%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f516f5-1382-4a7d-8733-abe44abfeafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW TO extract  dates (Year, Month, Monthname, Day, hour, weekdayname, dayofweek)\n",
    "df2['Year']=df2['Purchase_Date'].dt.year\n",
    "df2['Month']=df2['Purchase_Date'].dt.month\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M').astype(str): get yearmonth\n",
    "df2['MonthName']=df2['Purchase_Date'].dt.month_name()\n",
    "df2['Day']=df2['Purchase_Date'].dt.day\n",
    "df2['Hour']=df2['Purchase_Date'].dt.hour\n",
    "df2['weekdayname']=df2['Purchase_Date'].dt.day_name()\n",
    "df2['dayofweek']=df2['Purchase_Date'].dt.dayofweek\n",
    "df['Date_of_Purchase']= df['end'] - df['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef7cef-d7e4-4a6a-a059-6b33180585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[row, column]: Filters data based on a condition\n",
    "df2.loc[49, 'Product_Category']\n",
    "df.loc[:, 'Payment_Method']\n",
    "df.loc[5:10]\n",
    "L = df.loc[df[\"Science\"] < 50]\n",
    "grade_is_B = df.loc[df[\"Grade\"] == \"B\", [\"Name\", \"Age\", \"Math\",\t\"Science\", \"English\", \"Attendance\"]]\n",
    "\n",
    "h3= df.loc[df['number_of_reviews']<250, ['host_name','room_type','number_of_reviews','availability_365']]\n",
    "\n",
    "q=df.loc[df['Age']<=44]\n",
    "\n",
    "w=df.loc[df['Product_category'] != \"Books\"]]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc18ab7-04ef-4399-ac77-191d9d9cefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[start:end]: Selects rows by position (inclusive start, exclusive end)\n",
    "df2.iloc[0]\n",
    "df.iloc[:, 1].unique()\n",
    "df.iloc[0:5, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ec925-b09d-47a0-bc12-4cd623cbede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at['row_no', 'col_no']='change to value'\n",
    "df2.at[0, 'Payment_Method']\n",
    "df.iat['row_no', 'col_no']\n",
    "df.iat[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44d856-5571-4f02-a733-3e414d8579df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('condition'): Filters data using a query string\n",
    "df.query(\"Country == 'USA'\").sort_values(by='Total_Spent', ascending=False)[:10]\n",
    "x=df.query('Product_category !=\"Books\" ')\n",
    "x\n",
    "g3 = df.query(\"number_of_reviews < 250\").loc[:, ['host_name','room_type','number_of_reviews','availability_365']]\n",
    "#THE OPERATION ABOVE CAN ALSO BE DONE IN OTHER WAYS:\n",
    "p=df.query('Age<=44')\n",
    "p\n",
    "#ABOVE i have placed many conditions that have to be met by a particular row\n",
    "uk_us_clients=df.query(\"Country in ['UK', 'USA'] and Payment_Method in ['Cash', 'PayPal'] and Customer_Feedback == ['Neutral'] and Product_Category == 'Books'\")\n",
    "uk_us_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a794523-9e1e-4ee5-aa90-e6865bf7ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(like='Product_Category')\n",
    "df.filter(items=['Product_Category', 'Quantity'])#Use it to pick specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e8e94-8866-4f76-91eb-05cd40b0b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isin\n",
    "df[df[\"column\"].isin([value1, value2]): Filters rows where the column matches any of the specified value\n",
    "\n",
    "df['Quantity'].isin([4, 7, 3]).value_counts()\n",
    "#df[df[\"COUNTRY\"].isin[\"Canada\"]]\n",
    "df.isin({'COUNTRY': ['Canada']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba2cde-53ba-4155-bc58-88395d2e5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"column\"] > value]: Filters rows where column values are greater than a specified value\n",
    "\n",
    "df[(df['Quantity'] > 5) & (df['Price_per_Unit'] < 39)]\n",
    "\n",
    "df[(df['Discount_Applied'] == 'True') | (df['Customer_Feedback'] != 'Negative')]\n",
    "\n",
    "q3 = df[df['number_of_reviews'] < 250][['host_name','room_type','number_of_reviews','availability_365']]\n",
    "\n",
    "#DOUBLE CONDITIONS\n",
    "cond_1=((df[\"QUANTITY\"]>5)&(df[\"TOTAL_SPENT\"]>300))\n",
    "cond_2=((df[\"GENDER\"]=='Male')&(df[\"DISCOUNT_APPLIED\"]=='TRUE'))\n",
    "df[cond_1|cond_2]#conditional OR(|) you can also use AND (&)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36bfdf-02a0-45c3-a6da-7ebad00c0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TOTAL_MARKS\"] = df[\"Math\"] + df[\"Science\"] + df[\"English\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b191cd-aa98-4ddc-8ba3-acc0c6b3ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(['Country', 'Names'], ascending=[False,True])[:5]\n",
    "\n",
    "df2.sort_values(by='Date_of_Purchase', ascending=False, na_position='first')#na_position- stands for the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9737401-3228-44ab-9041-2faf1084b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "E=df2.eval('NEW_COL1 = Customer_ID % Age')\n",
    "E[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f521a14-a377-4f0c-bca5-37eb62750241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOW TO CREATE A NEW  COLUMN USING A FUNCTION \n",
    "def Age_Group(Age):\n",
    "    if Age <=20:\n",
    "        return 'Minor'\n",
    "    elif  21 <= Age < 40:\n",
    "        return 'Adult'\n",
    "    else:\n",
    "        return 'Old'\n",
    "df[\"Age_Group\"]=df[\"Age\"].apply(Age_Group)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46c8f5-1bc5-4692-aa9f-616ae87c24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Quantity'].apply(lambda x: x**2)\n",
    "df2.map(lambda x: len(str(x)))[:4]\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.2f}')\n",
    "df.apply(lambda row: row['a'] + row['b'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b760b8-0473-4cc5-b702-6871621bcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_row=next(df.iterrows())[1]\n",
    "pick_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67507da-45ea-4c75-9055-500a734186fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Spent'].clip(700, 900).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a0745-c034-4e7f-89c6-f5cd5e645060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.update\n",
    "new_column = pd.Series(['CHAL'], name='Name')\n",
    "df2.update(new_column)\n",
    "df2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff1c90-6680-448d-9ab7-ad5a75a2f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack()\n",
    "df.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5c6d7-b3ff-4a8b-af9a-2078b3968996",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = df[df[\"Science\"] < 50]\n",
    "U = df[df[\"Science\"] < 50][[\"Science\"]] #you can add df.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b415325-46e5-4476-89c0-c54e6211dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=df.drop(df[df['Product_Category']=='Books'].index)\n",
    "d\n",
    "#the code above is used to remove all books from the product category. Can be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096626b-ee3f-420c-be47-c94223e81f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#üìäPivot Tables & Cross Tabulation\n",
    "#Creates pivot table withaggregation\n",
    "pd.pivot_table(df, values='value',index='row_group', columns='column_group',aggfunc='sum')\n",
    "\n",
    "#: Creates a cross-tabulation of two columns (with margins)\n",
    "pd.crosstab(df['column1'], df['column2'], margins=True)\n",
    "\n",
    "#: Multiple aggregation functions in a pivot table\n",
    "df.pivot_table(values=\"value\", index=\"category\", aggfunc=[\"sum\", \"mean\", \"std\"])\n",
    "\n",
    "\n",
    "df2.pivot(index='Name', columns='Discount_Applied', values='Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e8cd3-6626-4c08-954f-956f638d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.explode('Customer_ID','Name')\n",
    "df3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d074a08-6b05-4139-8e49-3d583e5a5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.concat([df, df2], axis=0)\n",
    "df3.shape\n",
    "\n",
    "df4 = pd.concat([df, df3], axis=1)\n",
    "df4.shape\n",
    "\n",
    "merged = pd.merge(df, df2, on='id')\n",
    "merged_df = pd.merge(sales_df, marketing_df, on=\"Customer_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7b249-9f5c-4ec3-85d6-c126dd2c96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=df.groupby(\"neighbourhood_group\")[\"price\"].mean().sort_values(ascending=False).reset_index()\n",
    "q1\n",
    "\n",
    "df['ratio']=df['Females']/df['Males']\n",
    "df.groupby('County')['ratio'].max().nlargest(5).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d301e81-22fa-4785-b41f-c57dccc55090",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_room=df.groupby('room_type')['price'].mean().get('Private room').round(2)\n",
    "print(private_room)\n",
    "private_room= df.groupby('room_type')['price'].mean().loc['Private room']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d04d2-2a40-47ae-a1b4-c7d5c5729290",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîçHandling Outliers Using Z-Score\n",
    "from scipy import stats\n",
    " z_scores = stats.zscore(df[\"column\"]): Computes Z-scores for a column\n",
    " df = df[(z_scores < 3) & (z_scores > -3)]: Filters out outliers with Z-scores above 3 or below -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c31cd-ba2f-4d32-9b36-356a31bd83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîçHandling Outliers \n",
    "Q1 = df['Price(Kshs)'].quantile(0.25)\n",
    "Q3 = df['Price(Kshs)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = df[(df['Price(Kshs)'] < (Q1 - 1.5 * IQR)) | (df['Price(Kshs)'] > (Q3 + 1.5 * IQR))]\n",
    "print(outliers['Price(Kshs)'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc21613-e971-48e8-a9b0-f0e62695532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price(Kshs)'].quantile([0.1,0.25,0.5,0.75,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277b4ea-e7de-4ae9-93e7-0796500d46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'State' (e.g., \"NY\" ‚Üí \"New York\")\n",
    "state_mapping = {\"NY\": \"New York\", \"CA\": \"California\", \"TX\": \"Texas\"}  # Add more as needed\n",
    "df['State'] = df['State'].replace(state_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49802ca-774f-4e84-8076-ab7730561225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Samsung phones only\n",
    "samsung_phones = df[df['Phone_model'].str.contains('Samsung', case=False, na=False)]\n",
    "\n",
    "# Sort by price in descending order\n",
    "samsung_sorted = samsung_phones.nlargest(10, 'Price(Kshs)')\n",
    "\n",
    "display(samsung_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a99810-3ca3-4daa-9927-2097f0f5a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize sentiment\n",
    "def categorize_sentiment(score):\n",
    "    if score < -0.5: return 'Very Negative'\n",
    "    elif -0.5 <= score < 0: return 'Negative'\n",
    "    elif score == 0: return 'Neutral'\n",
    "    elif 0 < score <= 0.5: return 'Positive'\n",
    "    else: return 'Very Positive'\n",
    "\n",
    "df['Sentiment_Category'] = df['Sentiment'].apply(categorize_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82399b8-b5e1-4201-bfac-bf005aa566e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate profit margin (assuming Total sales is revenue and Unit_Price is cost)\n",
    "profit_margin = (df['Total sales'] - df['Unit_Price']) / df['Total sales'] * 100\n",
    "\n",
    "# Count values where profit margin is greater than 60%\n",
    "pf = (profit_margin > 60).value_counts()\n",
    "\n",
    "print(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29373d4-3d7f-4fe7-b9cc-00a2a68dfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand_model(text):\n",
    "    # Remove RAM/storage like '6GB128GB', '4GB64GB', etc.\n",
    "    text = re.sub(r'\\d+GB\\d*GB', '', text)\n",
    "    # Remove standalone years like 2019, 2020, etc.\n",
    "    text = re.sub(r'\\b20\\d{2}\\b', '', text)\n",
    "    return text.strip()\n",
    "df['Brand_Details'] = df['Phone Title'].apply(extract_brand_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d543c-1b65-4eb2-be24-819acfb195c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e90fd-387e-480a-aef6-b04d0a4c4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEXING\n",
    "df2=df.set_index(\"Rating\")#changes the indexcolumn of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96de21-286c-4b2a-8d01-2baf8a374021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is how, you divide the count of missing values in each column by the total number of rows, which gives the proportion of missing data for each column.\n",
    "#Then *100 gives as the percentage\n",
    "df.isnull().sum() /df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e2ff1-b44a-4118-a2b1-0c8ee4f27a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e721f13-d5dc-4686-9e5e-c307c958570d",
   "metadata": {},
   "source": [
    "## module specefic look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07d77e-0639-4a5c-ad23-bcc37a68bc57",
   "metadata": {},
   "source": [
    "###____________numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10806434-0b4e-4a72-826e-3ad1ecf54598",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ADVANCED NUMPY TECHNIQUES\n",
    "import numpy as np\n",
    "import time\n",
    "from numba import jit, vectorize\n",
    "import numpy.ma as ma\n",
    "from numpy.lib import recfunctions as rfn\n",
    "from numpy.random import Generator, PCG64\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# 1. Advanced array creation and manipulation\n",
    "print(\"===== ADVANCED ARRAY CREATION AND MANIPULATION =====\")\n",
    "\n",
    "# Creating arrays with specific memory layouts\n",
    "C_contiguous = np.ones((1000, 1000), order='C')  # Row-major (C-style)\n",
    "F_contiguous = np.ones((1000, 1000), order='F')  # Column-major (Fortran-style)\n",
    "print(f\"C-contiguous memory layout: {C_contiguous.flags['C_CONTIGUOUS']}\")\n",
    "print(f\"F-contiguous memory layout: {F_contiguous.flags['F_CONTIGUOUS']}\")\n",
    "\n",
    "# Advanced array views vs copies\n",
    "a = np.arange(10)\n",
    "b = a.view()  # View - changes to b affect a\n",
    "c = a.copy()  # Copy - changes to c don't affect a\n",
    "print(\"\\nViews vs Copies:\")\n",
    "print(f\"Original array: {a}\")\n",
    "b[0] = 99\n",
    "print(f\"After modifying view: {a}\")\n",
    "c[0] = 999\n",
    "print(f\"After modifying copy: {a}\")\n",
    "\n",
    "# Advanced reshaping with newaxis and broadcasting\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"\\nReshaping with newaxis:\")\n",
    "print(f\"Original: {x.shape}\")\n",
    "print(f\"With newaxis: {x[:, np.newaxis].shape}\")\n",
    "print(f\"Multiple newaxis: {x[:, np.newaxis, np.newaxis].shape}\")\n",
    "\n",
    "# Broadcasting visualization\n",
    "a = np.arange(3).reshape((3, 1))\n",
    "b = np.arange(3).reshape((1, 3))\n",
    "print(\"\\nBroadcasting visualization:\")\n",
    "print(f\"Shape a: {a.shape}, Shape b: {b.shape}\")\n",
    "print(f\"a:\\n{a}\")\n",
    "print(f\"b:\\n{b}\")\n",
    "print(f\"a + b (broadcasted):\\n{a + b}\")\n",
    "\n",
    "# 2. Advanced indexing and masking\n",
    "print(\"\\n===== ADVANCED INDEXING AND MASKING =====\")\n",
    "\n",
    "# Boolean masking with multiple conditions\n",
    "arr = np.random.normal(size=(5, 5))\n",
    "mask = (arr > 0) & (arr < 1)\n",
    "print(f\"Boolean mask:\\n{mask}\")\n",
    "print(f\"Masked values:\\n{arr[mask]}\")\n",
    "\n",
    "# Advanced integer indexing\n",
    "indices = np.array([[0, 1], [2, 3]])\n",
    "print(\"\\nAdvanced integer indexing:\")\n",
    "print(f\"Original array:\\n{arr}\")\n",
    "print(f\"Indexed values:\\n{arr[indices]}\")\n",
    "\n",
    "# Masked arrays for handling missing data\n",
    "data = np.random.rand(10)\n",
    "data[2:4] = np.nan\n",
    "masked_data = ma.masked_invalid(data)\n",
    "print(\"\\nMasked arrays:\")\n",
    "print(f\"Original with NaNs: {data}\")\n",
    "print(f\"Masked array: {masked_data}\")\n",
    "print(f\"Mean ignoring NaNs: {masked_data.mean()}\")\n",
    "\n",
    "# 3. Structured arrays and record arrays\n",
    "print(\"\\n===== STRUCTURED ARRAYS AND RECORD ARRAYS =====\")\n",
    "\n",
    "# Create a structured array\n",
    "dt = np.dtype([\n",
    "    ('name', 'U30'),\n",
    "    ('age', 'i4'),\n",
    "    ('height', 'f4'),\n",
    "    ('weight', 'f4'),\n",
    "    ('is_active', 'bool')\n",
    "])\n",
    "\n",
    "people = np.array([\n",
    "    ('Alice', 28, 165.5, 55.0, True),\n",
    "    ('Bob', 35, 180.0, 85.5, False),\n",
    "    ('Charlie', 42, 175.2, 70.1, True),\n",
    "    ('Diana', 31, 168.0, 65.2, True)\n",
    "], dtype=dt)\n",
    "\n",
    "print(\"Structured array:\")\n",
    "print(people)\n",
    "\n",
    "# Field access and filtering\n",
    "print(\"\\nAccessing fields:\")\n",
    "print(f\"Names: {people['name']}\")\n",
    "print(f\"Active people: {people[people['is_active']]}\")\n",
    "\n",
    "# Calculate BMI using structured array fields\n",
    "bmi = people['weight'] / ((people['height']/100) ** 2)\n",
    "print(f\"\\nCalculated BMI: {bmi}\")\n",
    "\n",
    "# Adding fields to structured arrays\n",
    "people_with_bmi = rfn.append_fields(people, 'bmi', bmi, usemask=False)\n",
    "print(\"\\nStructured array with added BMI field:\")\n",
    "print(people_with_bmi)\n",
    "\n",
    "# 4. Advanced ufuncs and performance optimization\n",
    "print(\"\\n===== ADVANCED UFUNCS AND PERFORMANCE OPTIMIZATION =====\")\n",
    "\n",
    "# Custom ufunc with Numba\n",
    "@vectorize(['float64(float64, float64)'], target='parallel')\n",
    "def custom_sigmoid(x, alpha):\n",
    "    return 1.0 / (1.0 + np.exp(-alpha * x))\n",
    "\n",
    "# Performance comparison\n",
    "x = np.linspace(-10, 10, 1000000)\n",
    "alpha = 1.5\n",
    "\n",
    "# Standard numpy implementation\n",
    "def numpy_sigmoid(x, alpha):\n",
    "    return 1.0 / (1.0 + np.exp(-alpha * x))\n",
    "\n",
    "# Time comparison\n",
    "start = time.time()\n",
    "result1 = numpy_sigmoid(x, alpha)\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result2 = custom_sigmoid(x, alpha)\n",
    "numba_time = time.time() - start\n",
    "\n",
    "print(f\"Numpy implementation time: {numpy_time:.6f} seconds\")\n",
    "print(f\"Numba vectorized time: {numba_time:.6f} seconds\")\n",
    "print(f\"Speedup: {numpy_time/numba_time:.2f}x\")\n",
    "\n",
    "# 5. Advanced random number generation\n",
    "print(\"\\n===== ADVANCED RANDOM NUMBER GENERATION =====\")\n",
    "\n",
    "# Modern random generators (NumPy 1.17+)\n",
    "rg = Generator(PCG64(12345))  # PCG64 is more statistically robust than older generators\n",
    "\n",
    "# Generate from different distributions\n",
    "normal = rg.normal(loc=0, scale=1, size=1000)\n",
    "poisson = rg.poisson(lam=5, size=1000)\n",
    "multinomial = rg.multinomial(n=10, pvals=[0.2, 0.3, 0.5], size=5)\n",
    "\n",
    "print(\"Modern random number generation:\")\n",
    "print(f\"Normal distribution (first 5): {normal[:5]}\")\n",
    "print(f\"Poisson distribution (first 5): {poisson[:5]}\")\n",
    "print(f\"Multinomial distribution:\\n{multinomial}\")\n",
    "\n",
    "# 6. Sparse arrays for memory efficiency\n",
    "print(\"\\n===== SPARSE ARRAYS =====\")\n",
    "\n",
    "# Create a mostly-zero dense array\n",
    "dense = np.zeros((1000, 1000))\n",
    "dense[0, 0] = 1\n",
    "dense[10, 10] = 2\n",
    "dense[999, 999] = 3\n",
    "\n",
    "# Convert to different sparse formats\n",
    "csr = sparse.csr_matrix(dense)\n",
    "csc = sparse.csc_matrix(dense)\n",
    "coo = sparse.coo_matrix(dense)\n",
    "\n",
    "print(f\"Dense array memory: {dense.nbytes / 1024:.2f} KB\")\n",
    "print(f\"CSR format memory: {csr.data.nbytes + csr.indptr.nbytes + csr.indices.nbytes:.2f} bytes\")\n",
    "print(f\"CSC format memory: {csc.data.nbytes + csc.indptr.nbytes + csc.indices.nbytes:.2f} bytes\")\n",
    "print(f\"COO format memory: {coo.data.nbytes + coo.row.nbytes + coo.col.nbytes:.2f} bytes\")\n",
    "\n",
    "# Sparse array operations\n",
    "sparse_result = csr @ csc.T  # Matrix multiplication with sparse arrays\n",
    "print(f\"Sparse matrix multiplication shape: {sparse_result.shape}\")\n",
    "\n",
    "# 7. Advanced linear algebra\n",
    "print(\"\\n===== ADVANCED LINEAR ALGEBRA =====\")\n",
    "\n",
    "# SVD decomposition\n",
    "A = np.random.rand(5, 3)\n",
    "U, S,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03aed6-b21c-47b9-8b8c-b09d6c14fd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8993810f-8b79-40f5-aa73-85f48286f1b9",
   "metadata": {},
   "source": [
    "###_____________pandas_____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b93464-5d16-4d53-9d2f-1eb2ab1333b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED PANDAS TECHNIQUES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay, MonthEnd, BDay\n",
    "import swifter  # For optimized apply operations\n",
    "import pandas_profiling as pp\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=SettingWithCopyWarning)\n",
    "\n",
    "# Create sample data for demonstrations\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('20220101', periods=1000)\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 1000),\n",
    "    'subcategory': np.random.choice(['X', 'Y', 'Z'], 1000),\n",
    "    'value1': np.random.randn(1000),\n",
    "    'value2': np.random.randn(1000) * 2 + 1,\n",
    "    'value3': np.random.randint(0, 100, 1000),\n",
    "    'text': np.random.choice(['apple', 'banana', 'orange', 'grape', 'kiwi'], 1000)\n",
    "})\n",
    "\n",
    "# 1. Advanced DataFrame Creation and Manipulation\n",
    "print(\"===== ADVANCED DATAFRAME CREATION AND MANIPULATION =====\")\n",
    "\n",
    "# Creating DataFrames from different sources\n",
    "# From dictionary of Series\n",
    "s1 = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "s2 = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "df_from_series = pd.DataFrame({'col1': s1, 'col2': s2})\n",
    "print(\"DataFrame from Series:\")\n",
    "print(df_from_series)\n",
    "\n",
    "# From nested dictionary\n",
    "nested_dict = {\n",
    "    'A': {'a': 1, 'b': 2},\n",
    "    'B': {'a': 3, 'b': 4, 'c': 5}\n",
    "}\n",
    "df_from_nested = pd.DataFrame(nested_dict)\n",
    "print(\"\\nDataFrame from nested dictionary:\")\n",
    "print(df_from_nested)\n",
    "\n",
    "# From records (list of dicts)\n",
    "records = [\n",
    "    {'col1': 1, 'col2': 'a'},\n",
    "    {'col1': 2, 'col2': 'b', 'col3': True},\n",
    "    {'col1': 3}\n",
    "]\n",
    "df_from_records = pd.DataFrame.from_records(records)\n",
    "print(\"\\nDataFrame from records:\")\n",
    "print(df_from_records)\n",
    "\n",
    "# Advanced column manipulation\n",
    "# Add multiple columns at once\n",
    "df['value4'], df['value5'] = np.random.randn(2, 1000)\n",
    "\n",
    "# Column operations with eval (more efficient for large DataFrames)\n",
    "df.eval('value6 = value1 * value2 + value3', inplace=True)\n",
    "print(\"\\nUsing eval for efficient column operations:\")\n",
    "print(df[['value1', 'value2', 'value3', 'value6']].head())\n",
    "\n",
    "# 2. Advanced Indexing and Selection\n",
    "print(\"\\n===== ADVANCED INDEXING AND SELECTION =====\")\n",
    "\n",
    "# Set multi-level index\n",
    "df_multi = df.set_index(['category', 'subcategory'])\n",
    "print(\"Multi-level indexed DataFrame:\")\n",
    "print(df_multi.head())\n",
    "\n",
    "# Cross-section (xs) for multi-level selection\n",
    "print(\"\\nCross-section selection for level 'category' = 'A':\")\n",
    "print(df_multi.xs('A', level='category').head())\n",
    "\n",
    "# Advanced boolean indexing with query\n",
    "print(\"\\nUsing query for complex filtering:\")\n",
    "result = df.query('value1 > 0 and value2 < 0 and category in [\"A\", \"B\"]')\n",
    "print(f\"Filtered results shape: {result.shape}\")\n",
    "print(result.head())\n",
    "\n",
    "# Using loc with callable\n",
    "print(\"\\nUsing loc with callable:\")\n",
    "result = df.loc[lambda x: (x['value3'] > 50) & (x['value1'] < 0)]\n",
    "print(f\"Filtered results shape: {result.shape}\")\n",
    "print(result.head())\n",
    "\n",
    "# 3. Advanced Data Cleaning and Transformation\n",
    "print(\"\\n===== ADVANCED DATA CLEANING AND TRANSFORMATION =====\")\n",
    "\n",
    "# Create some messy data\n",
    "messy_df = df.copy()\n",
    "messy_df.loc[10:20, 'value1'] = np.nan\n",
    "messy_df.loc[30:40, 'value2'] = np.nan\n",
    "messy_df.loc[50:60, 'category'] = None\n",
    "messy_df.loc[100:110, 'text'] = None\n",
    "\n",
    "# Advanced missing value imputation\n",
    "# Using interpolate with different methods\n",
    "methods = ['linear', 'quadratic', 'cubic', 'spline', 'polynomial', 'time']\n",
    "for method in methods[:3]:  # Using first 3 methods for brevity\n",
    "    col_name = f'value1_{method}'\n",
    "    messy_df[col_name] = messy_df['value1'].interpolate(method=method)\n",
    "\n",
    "print(\"Advanced interpolation methods:\")\n",
    "print(messy_df.loc[5:25, ['value1'] + [f'value1_{m}' for m in methods[:3]]].head(10))\n",
    "\n",
    "# Using fillna with different strategies\n",
    "messy_df['value2_ffill'] = messy_df['value2'].fillna(method='ffill')\n",
    "messy_df['value2_bfill'] = messy_df['value2'].fillna(method='bfill')\n",
    "messy_df['value2_mean'] = messy_df['value2'].fillna(messy_df['value2'].mean())\n",
    "messy_df['value2_median'] = messy_df['value2'].fillna(messy_df['value2'].median())\n",
    "\n",
    "print(\"\\nAdvanced fillna strategies:\")\n",
    "print(messy_df.loc[25:45, ['value2', 'value2_ffill', 'value2_bfill', 'value2_mean', 'value2_median']].head(10))\n",
    "\n",
    "# Advanced categorical data handling\n",
    "# Create ordered categories\n",
    "cat_type = CategoricalDtype(categories=['D', 'C', 'B', 'A'], ordered=True)\n",
    "df['category_ordered'] = df['category'].astype(cat_type)\n",
    "\n",
    "print(\"\\nOrdered categorical data:\")\n",
    "print(df[['category', 'category_ordered']].head())\n",
    "print(f\"Category 'A' > 'B': {df['category_ordered'].iloc[0] > df['category_ordered'].iloc[1]}\")\n",
    "\n",
    "# 4. Advanced GroupBy Operations\n",
    "print(\"\\n===== ADVANCED GROUPBY OPERATIONS =====\")\n",
    "\n",
    "# Custom aggregation functions\n",
    "def q25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def iqr(x):\n",
    "    return x.quantile(0.75) - x.quantile(0.25)\n",
    "\n",
    "# Named aggregation\n",
    "result = df.groupby(['category', 'subcategory']).agg(\n",
    "    mean_val1=('value1', 'mean'),\n",
    "    median_val2=('value2', 'median'),\n",
    "    iqr_val3=('value3', iqr),\n",
    "    count=('value1', 'count')\n",
    ")\n",
    "\n",
    "print(\"Named aggregation with custom functions:\")\n",
    "print(result.head())\n",
    "\n",
    "# Transform vs Agg\n",
    "# Transform returns same-sized DataFrame\n",
    "normalized = df.groupby('category')['value1'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "df['value1_normalized'] = normalized\n",
    "\n",
    "print(\"\\nUsing transform for normalization:\")\n",
    "print(df[['category', 'value1', 'value1_normalized']].head(10))\n",
    "\n",
    "# Filter groups based on a condition\n",
    "filtered = df.groupby('category').filter(lambda x: x['value1'].mean() > 0)\n",
    "print(f\"\\nFiltered groups where mean(value1) > 0: {filtered['category'].unique()}\")\n",
    "\n",
    "# 5. Advanced Time Series Analysis\n",
    "print(\"\\n===== ADVANCED TIME SERIES ANALYSIS =====\")\n",
    "\n",
    "# Set date as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ca6b0-68e8-405a-bcde-43743422eafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cee7d4-ffbb-4d8b-bd2b-411e2c6827dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112601d4-64f9-43fe-b83c-3985123937a7",
   "metadata": {},
   "source": [
    "## **thank you so much**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f4a6e-a293-49ed-832a-d1686f5a7ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
